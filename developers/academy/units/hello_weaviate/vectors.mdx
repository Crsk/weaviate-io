---
title: Vectors and vector search
sidebar_position: 20
---

## Vectors and meaning

:::warning TODO
Intro video here
:::

### What is a vector?

In mathematics and sciences, the word "vector" can be used refer to many things. 

A vector in our (data science) context is a set of numbers. If you have seen words like "tensor", or "array", in a similar context they generally refer to the same thing - with perhaps slight differences depending on whether they might be referring to particular software libraries, or the number of dimensions.

To be precise, a vector is **an one-dimensional array of numbers**, such as `1, 5, 5`, `0.591, -0.132, 0, 0.105`, or `0, 0, 0, 0, 0, 0`. 

A vector is typically shown as its constituent numbers wrapped in square brackets. So, the above vectors may be typically represented as `[1, 5, 5]`, `[0.591, -0.132, 0, 0.105]` and `[0, 0, 0, 0, 0, 0]` respectively.

### How can a vector represent meaning?

A vector itself is a relatively small concept. It's a collection, or a one-dimensional array, of numbers. This may not seem like much in itself. But the real power of vectors comes from the fact that they can be used to represent meaning. So how can a series of numbers represent meaning?

#### Everyday examples of vectors

One way to think about it is that each number in a vector represents a particular aspect of meaning. Then, the vector itself represents the combination of all of these aspects. 

In fact, many of you may have already used vectors to describe aspects of meaning. For example, you may have used a vector to:
- Describe the position of an object in space by sending a location pin to somebody. This is a vector that contains numbers for the latitude and longitude, or
- Describe the color of an object by a vector of numbers that represent the red, green and blue (and perhaps transparency) components of the color.

In each of these examples, each number in the vector represents a particular aspect of the object, such as its longitude or how green the object is. The entire vector combines these aspects to represent the "meaning", such as its location or its color.

Extending this idea, the above two vectors might be combined to one vector that contains its latitude, longitude, red, green, blue and transparency. This vector would then represent the object's location and color. 

In other words, a vector can represent any property of an object by enumerating a property such as how far north it is, how far west it is, how red it is, how green it is, how blue it is, and how transparent it is.

#### Vectors for semantic representation

The semantic use of vectors extends this idea further to represent meaning of words, sentences, or even entire documents. 

Again, each number in a vector represents a particular aspect of meaning. Imagine how you would try to represent words like "lion", "grapefruit", "cat" and "cow". You might have each numbers within a vector capture different aspects, such as:
- How "furry" the object is
- How "big" the object is
- How "dangerous" the object is
- How "edible" the object is

And so on.

Then, the vector itself would represent the combination of all of these aspects. So, a vector for the word "lion" might be `[0.8, 0.9, 0.9, 0.1]`, while a vector for the word "grapefruit" might be `[0.1, 0.3, 0.1, 0.9]`.

Each number in a vector represents a particular aspect of meaning, and the entire vector represents the combination of all of these aspects.

A "real" vector that is generated by modern language models are simply longer. A vector that is used to represent the word "lion" might have a much longer length, such as hundreds or thousands of numbers, but the idea is the same. Each number represents some aspect of a meaning, and the resulting entire vector can be used to represent the meaning of the word.

#### How are vectors actually generated?

Vectors are generated by machine learning models. These models are trained on large amounts of data, and are able to learn the relationships between words and their meanings.

These models use the training data to infer relationships between words and their meanings. For example, a model might learn that the word "lion" is similar to the word "tiger" because they are both big cats. It might also learn that the word "lion" is similar to the word "cow" because they are both animals. And then, they use this knowledge to find-tune individual numbers in vectors such that they best represent the meaning of the words.

This is why choosing the right model is so important. The model you choose will have a significant impact on the quality of the vectors, and what it means to be "similar" to another word. 

A model that is trained based on how words are used in everyday English is going to be different from a model that is trained on how words are used in scientific papers, or in particular domains such as medicine or law.

We will cover this in more detail elsewhere in the course, in regards to how to choose the right model for your use case.

<Quiz questions={vectorDefinition} />

<Quiz questions={vectorsAndMeaning} />

<Quiz questions={vectorGeneration} />

## About vector search

:::warning TODO
Intro video here
:::

### How does vector search work?

Earlier on, we mentioned that a vector can be used to represent the meaning of a word. In an earlier section, we also mentioned that a vector search is a fantastic way of carrying out similarity-based searches. 

You can probably see how these two ideas converge. A vector search relies on these numerical representation of meaning to carry out similarity-based searches. The closer two vectors are to each other, the more similar the words they represent.

A vector search therefore works by providing a vector-based query object to a vector search engine. The vector search engine then returns a list of results that are most similar to the query object.

It may be that a vector search engine takes as input an object such as a text passage or an image which it converts to a vector. Or it may receive a vector as the query object directly. 

### How is vector similarity measured?

Broadly speaking, vector similarity measures how close the two sets of numbers that make up each vector are to each other. Depending on your relationship with linear algebra, this may sound like a very abstract concept. 

But the intuition is this: the closer the two vectors are to each other, the more similar the words they represent.

In practice, there are many different ways to measure vector similarity. The most common methods include: 
- Cosine similarity
- Euclidean distance
- Dot product
- Manhattan distance

The choice of similarity measure will depend on the use case and the model. In Weaviate, the default metric unless specified is `cosine similarity`. We will cover these choices in more detail in another section.

### What is a semantic search?

"Semantic" refers to something being "related to meaning". It is often used in the field of linguistics (study of languages) to contrast to "syntactic", which refers to structure or grammar. Thus "semantic search" refers to a search that is based on meaning.

Vector search is currently the dominant way of performing semantic searches. Accordingly, we will use the terms "semantic search" and "vector search" interchangeably in this course unless otherwise specified.

### What is a lexical search?

Traditional search systems use "lexical" searches, based on a "vocabulary" of the search system or database at hand.

A lexical search relies on matching exact strings or substrings and using various operators to manipulate sets of information that either meets or does not meet certain criteria. 

For example, looking through a database of recipes to retrieve all objects containing the ingredient "chili" is an example of a lexical search. You might typically combine multiple conditions, so that for example the below entry would look for entries that where added after the year 2010.

```sql
SELECT * FROM recipe_table WHERE (ingredients LIKE '%chili%') AND (entry_year > 2010)
```

While this enquiry will return an exact answer, it suffers from being  inflexible. 

For one, it would not retrieve recipes where another string is used (e.g. `ghost pepper`) that is synonymous (has the same meaning) with chili. It would also not handle typos well, for example if the entry was mislabelled as `chilli`, or if the entry was mistyped as such. 

On the other hand, semantic searches manage these issues with a little more nuance.

:::note Jargon ("lexical", "semantic", "syntactic")
We try to avoid jargon unless necessary. But where we do use them, we will include explanations for those who have not encountered them.

Because vector search as we use them has some roots in *natural language processing*, terms like *lexical* and *semantic* are commonly used. These are terms that come from linguistics which are used commonly in vector search context. The term *vector* comes from the world of mathematics, as you will see later on. ðŸ¤“
:::

### Vector search vs lexical searches

When facing a query of a recipes database with the word `chili` for ingredients, a vector search engine returns entities that include that exact word in the recipe data. 

This means that a search for `chili` would include in its results entities with semantically related terms such as `ghost pepper`, `jalapeno`, `habanero` or `carolina reaper`, all of which would be ignored by a lexical search.

A modern vector search engine can also take the context into account, such as the context of a word as a part of its parent sentence. So a vector search will differentiate between whether the recipe calls for these ingredients, or whether the recipe asks the cook to "*not* use any chili". 

Also importantly, a vector search engine can determine the degree of similarity between items, such as between a query and each result. Accordingly, the results can be ranked by a meaningful metric that describes the degree of similarity, thereby improving the chance of finding more relevant results faster.

<Quiz questions={lexicalSearch} />

<Quiz questions={vectorSearchMethod} />

## Types of vector representations

:::warning TODO
Intro video here
:::

When we talk about vectors, we are typically referring to vectors that are derived by machine-learning models. More specifically, we refer to vectors that are derived from neural networks, called "dense" vectors.

However, there are other vector representations that are used to represent meaning, especially in relation to textual meaning. They include:

- One-hot encoding
- TF-IDF (term frequency-inverse document frequency) vectors
- BM25 vectors

Let's take a brief look at each one, as well as dense vectors.

### One-hot encoding

One-hot encodings represent text as a collection of 0s and 1s, where each 1 represents the presence of a word in the text. Sometimes this is also referred to as a "bag of words" representation.

Accordingly, this representation ends up being very sparse, with most of the vector being 0s. This is because most words are not present in a given text.

A limitation of this method is that it is not able to capture similarity of words, as each word is simply represented as being present or not present. Additionally, it is not able to take into account the relative importance of words in a text.

### TF-IDF vectors

A TF-IDF representation improves on the one-hot encoding by taking into account the relative importance of words in a text.

TF-IDF stands for "term frequency-inverse document frequency". It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.

The TF-IDF value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus. This means that common words across all documents such as "the", "a", "is" and "are" are penalized, whereas words that are rare across all documents are given more weight.

Intuitively, this means that TF-IDF is able to capture the relative importance of words in a text by weighting rare words more heavily.

### BM25 vectors

BM25 vectors are similar to TF-IDF vectors, but they are able to take into account the length of a document. This is important because longer documents will have more words, and therefore a higher TF-IDF score, even if they are not more relevant than shorter documents.

Accordingly, BM25 vectors normalize the TF-IDF score by the length of the document.

### Dense Vectors

#### Word vectors

Word vectors are a type of vector representation that is derived from neural networks. They are able to capture the meaning of words by learning the context in which they appear.

Models such as "word2vec" and "GloVe" popularized this type of representational learning. One key shortcoming of word-based vectors is that they are not able to take into account local context, such as the context of a word as a part of its parent sentence.

This meant that where words need to be disambiguated, such as in the case of homonyms, word vectors were not able to capture the meaning of the word in the context of the sentence. (For example, the word "bank" can mean a financial institution or a river bank.)

:::note Word vectors + weighting
Word vectors in a text can be combined with a weighting method such as TF-IDF or BM25 to capture the relative importance of words in the text. The resulting vector can be used to represent the entire text.
:::

#### Transformer-derived vectors

Most modern vector search engines use vectors that are derived from what are called "transformer" models. 

Transformers are a type of neural network that are able to take into account the context of its parent sentence in determining the meaning of each word. This means that they are able to disambiguate words that have multiple meanings, such as the word "bank" in the example above.

Their current key shortcoming is their resource-intensive nature, especially as the input size (e.g. text length) increases. 

<Quiz questions={sparseOrDense} />

<Quiz questions={wordVecVsTransformer} />

import Quiz from '/src/components/Academy/quiz.js' 
const vectorDefinition = [
  {
    questionText: 'What is a vector in the context of data science?',
    answerOptions: [
      { answerText: 'An array of letters.', isCorrect: false, feedback: 'It is not an array of letters.'},
      { answerText: 'An array of numbers.', isCorrect: true, feedback: 'Note: Typically they are floating point numbers.'},   
      { answerText: 'An array of symbols.', isCorrect: false, feedback: 'It is not an array of symbols.'},
      { answerText: 'A number with direction and magnitude.', isCorrect: false, feedback: 'This is a definition in science or mathematics, but not so much in the data science context.'},
    ],
  },     
];
const vectorsAndMeaning = [
  {
    questionText: 'Which of these is not a good example of vectors representing meaning?',
    answerOptions: [
      { answerText: 'An RGB value representing color.', isCorrect: false, feedback: 'Actually, this is a good example. An RGB value uses three numbers for (R)ed, (G)reen and (B)lue to represent color.'},
      { answerText: 'A location coordinate representing longitude, latitude and altitude.', isCorrect: false, feedback: 'Actually, this is a good example. These three numbers would accurately and precisely describe the location.'},   
      { answerText: 'A 1024-dimensional vector generated by a language model to represent the word "butterfly".', isCorrect: false, feedback: 'Actually, this is a good example. This is in fact a typical way in which modern vector would be generated.'},
      { answerText: 'None, these are all good examples.', isCorrect: true, feedback: 'All of these are perfectly reasonable examples of vectors representing meaning.'},
    ],
  },     
];
const vectorGeneration = [
  {
    questionText: 'How are vectors generated?',
    answerOptions: [
      { answerText: 'Manually inputting numbers into an array.', isCorrect: false, feedback: 'This might be theoretically possible, but highly impractical.'},
      { answerText: 'Training machine learning models on large amounts of data.', isCorrect: true, feedback: 'This is typically how they are generated.'},   
      { answerText: 'Automatically generating random numbers.', isCorrect: false, feedback: 'While this would work, these vectors would not be "meaningful". Interestingly, this is how they are typically "initialized" during training.'},
      { answerText: 'Copying numbers from other vectors.', isCorrect: false, feedback: 'This would work, but would not produce unique vectors.'},
    ],
  },     
];
const lexicalSearch = [
  {
    questionText: 'What type of search is based on word or token matches?',
    answerOptions: [
      { answerText: 'Lexical search.', isCorrect: true, feedback: 'They are called this because they are "lexical" or vocabulary-based searches.'},
      { answerText: 'Vector search.', isCorrect: false, feedback: 'Vector search uses similarity in vectors.'},   
      { answerText: 'Semantic search.', isCorrect: false, feedback: 'Semantic search looks for similarity in meaning.'},
      { answerText: 'None of the above.', isCorrect: false, feedback: 'The truth is out there.'},
    ],
  },     
];
const vectorSearchMethod = [
  {
    questionText: 'How does vector search return relevant results?',
    answerOptions: [
      { answerText: 'It looks for objects with the highest vector similarity.', isCorrect: true, feedback: 'Because vectors capture meaning, objects with the highest similarity are the most relevant results.'},
      { answerText: 'It looks for objects with the smallest vectors.', isCorrect: false, feedback: 'The magnitude of the vector is largely not relevant.'},   
      { answerText: 'It looks for objects with the largest vectors.', isCorrect: false, feedback: 'The magnitude of the vector is largely not relevant.'},
      { answerText: 'It looks for objects with the highest character overlap.', isCorrect: false, feedback: 'This is not quite right. It is not about character overlap, but rather about meaning.'},
    ],
  },     
];
const sparseOrDense = [
  {
    questionText: 'From the folloowing, select the correct statement about sparse and dense vectors.',
    answerOptions: [
      { answerText: 'One-hot encoding & word vectors: sparse, Transformer-derived: dense.', isCorrect: false, feedback: 'Word vectors are not sparse.'},
      { answerText: 'Document vector generated from BM25-weighted word vectors: sparse.', isCorrect: false, feedback: 'Word vectors are dense. Accordingly, a document vector generated by weighting BM25 scores are also dense.'},      
      { answerText: 'One-hot encoding: sparse, TF-IDF based bag of words: dense.', isCorrect: false, feedback: 'Bag-of-words vectors are sparse. Accordingly, a vector that is based on TF-IDF weighting is also sparse.'},       
      { answerText: 'One-hot encoding: sparse, Word vectors & transformer-derived: dense.', isCorrect: true, feedback: 'This is the only correct answer.'},   
    ],
  },     
];
const wordVecVsTransformer = [
  {
    questionText: 'Select the correct statement.',
    answerOptions: [
      { answerText: 'One-hot encoding & word vectors: sparse, Transformer-derived: dense.', isCorrect: false, feedback: 'Word vectors are not sparse.'},
      { answerText: 'Document vector generated from BM25-weighted word vectors: sparse.', isCorrect: false, feedback: 'Word vectors are dense. Accordingly, a document vector generated by weighting BM25 scores are also dense.'},      
      { answerText: 'One-hot encoding: sparse, TF-IDF based bag of words: dense.', isCorrect: false, feedback: 'Bag-of-words vectors are sparse. Accordingly, a vector that is based on TF-IDF weighting is also sparse.'},       
      { answerText: 'One-hot encoding: sparse, Word vectors & transformer-derived: dense.', isCorrect: true, feedback: 'This is the only correct answer.'},   
    ],
  },     
];