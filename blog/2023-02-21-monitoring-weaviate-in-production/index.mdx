---
title: Monitoring Weaviate in Production
slug: monitoring-weaviate-in-production
authors: jt
date: 2023-02-21
tags: []
---

Weaviate aims to be easy to monitor and observe by following a cloud native approach. When
deploying Weaviate you can

1. Publish Prometheus metrics to the standard `/metrics` endpoint,

2. Use built-in [liveness and readiness](https://weaviate.io/developers/weaviate/api/rest/well-known) checks,

3. Configure settings via environment variables,

4. Easily deploy in Kubernetes using the [helm charts](https://github.com/weaviate/weaviate-helm).

There is existing documentation on [the exported metrics](https://weaviate.io/developers/weaviate/configuration/monitoring)
which also has [an example](https://github.com/weaviate/weaviate-examples/tree/main/monitoring-prometheus-grafana) for
how to use a Prometheus instance for metrics.

One common question though is how can I use Weaviate with my existing observability stack?

This article describes two approaches using either Grafana agent or Datadog agent to scrape these metrics. It also provides a list of important metrics to monitor.

## Prerequisites

It is assumed that you have already deployed Weaviate. By default Prometheus monitoring is disabled so enable with environment setting:

```sh
PROMETHEUS_MONITORING_ENABLED=true
```

Weaviate will then publish Prometheus metrics on port 2112.

*Note:* If you are using Weaviate 1.17 or lower, you may want to upgrade to 1.18 before enabling Prometheus metrics. The reason being
Weaviate previously published many histograms which has since been [replaced](https://github.com/weaviate/weaviate/pull/2605) by summaries for performance reasons. Additionally be careful enabling prometheus metrics if you have thousands of classes you
may end up with high cardinality labels due to metrics being produced per class.

## Grafana agent

For the first approach we will use the open-source [Grafana agent](https://grafana.com/docs/grafana-cloud/data-configuration/agent/). In this case we will show writing to Grafana Cloud for hosted metrics.


### Steps to install

1. Install Grafana agent in your target environment following the [set-up guide](https://grafana.com/docs/agent/latest/set-up/).

2. Configure the Grafana `agent.yaml` to include a scrape job called `weaviate`. This will autodiscover Weaviate pods 
 in Kubernetes. The `app=weaviate` label is automatically added by the Weaviate helm chart.

```yaml
metrics:
    configs:
    - name: weaviate
    # reference https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config
    scrape_configs:
    - job_name: weaviate
        scrape_interval: 30s
        scheme: http
        metrics_path: /metrics
        kubernetes_sd_configs:
        - role: pod
        selectors:
        - role: "pod"
            label: "app=weaviate"
    remote_write:
    - url: <Your Grafana.com prometheus push url>
        basic_auth:
        username: <Your Grafana.com userid>
        password: <Your Grafana.com API Key>
```

3. Validate that you are receiving data by going to explore and running the following PromQL query in Grafana.

```
go_memstats_heap_inuse_bytes{job="weaviate"}
```
### Dashboards

One benefit of using Grafana agent and pushing to managed Mimir / Prometheus instance is you can reuse the existing dashboards that come with Weaviate.

Steps to import these dashboards:

1. Download and import the [prexisting dashboards](https://github.com/weaviate/weaviate/tree/master/tools/dev/grafana/dashboards).

2. If using Grafana Cloud hosted Prometheus you will need to patch the dashboards to change the datasource uid to be `grafanacloud-prom` as below.

```sh
sed 's/"uid": "Prometheus"/"uid": "grafanacloud-prom"/g' querying.json > querying-patched.json
```

3. Dashboards should now be visible!

![query latency](./img/query-latency.png)

## Datadog

Datadog is another popular solution for observability and the Datadog agent has support for scraping Prometheus metrics.

### Steps to install

1. Install the datadog agent, for this example installation was done using [Helm](https://docs.datadoghq.com/containers/kubernetes/installation/?tab=helm).

2. Provide datadog-values.yml including the following:

```yaml
datadog:
# Note DD_KUBELET_TLS_VERIFY only needs to be set if running a local docker kubernetes cluster
#  env:
#  - name: DD_KUBELET_TLS_VERIFY
#    value: "false"
  clusterName: weaviate-deployment
  prometheusScrape:
    enabled: true
    serviceEndpoints: true
    additionalConfigs:
      - configurations:
        - max_returned_metrics: 20000
          min_collection_interval: 30
```

3. Customise the Weaviate [helm chart](https://github.com/weaviate/weaviate-helm/blob/80346f0f1e1f22ad84a899b5f9e12f44be3ee809/weaviate/values.yaml#L730) to have annotations `prometheus.io/scrape` and `prometheus.io/port`

```yaml
# Pass any annotations to Weaviate pods
annotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "2112"
```

4. Validate metrics are available. `go_memstats_heap_inuse_bytes` should always be present even with an empty schema.

![datadog summary](./img/datadog-summary.png)


## Key metrics

Below are some key metrics to monitor. Standard CPU, Disk, Network metrics are also useful as are [Kubernetes
events](https://grafana.com/blog/2023/01/23/how-to-use-kubernetes-events-for-effective-alerting-and-monitoring/).
You will need to load and query Weaviate before seeing some metrics.

### Heap usage

```
go_memstats_heap_inuse_bytes
```

Expectation is the memory will have the standard jagged pattern underload but that memory will drop periodically
due to go garbage collection. If memory is not dropping and staying very close to the [GOMEMLIMIT](https://weaviate.io/blog/gomemlimit-a-game-changer-for-high-memory-applications) you may need to increase resources.

### Batch Latency

```
rate(batch_durations_ms_sum[30s])/rate(batch_durations_ms_count[30s])
```

Dependent on dataset and whether you are using a module for vectorizing data, but monitoring relative batch
latency can give an indication if there is a problem with indexing data. This metric has a label `operation` which
allows you to see how long object, vector, and inverted index sub operations take.

If you are doing batch deletes then the corresponding `batch_delete_durations_ms` metric will also be useful.

### Object Latency

Generally batch indexing is recommended but there are situations where you would do single `PUT` or `DELETE` operations
such as handling live changes from a user in an application. In this case you will want to monitor the object latency
instead.

```
rate(objects_durations_ms_sum{operation="put"}[30s])/rate(objects_durations_ms_latency{operation="put"}[30s])
```

### Query Latency and Rate

The latency and number or queries per second are also important, particularly for monitoring usage patterns.

```
rate(queries_durations_ms_sum[30s])/rate(queries_durations_ms_count[30s])
rate(queries_durations_ms_count)[30s]
```


# Other integrations

Other products that have integrations for prometheus can also be used

- Elastic https://docs.elastic.co/integrations/prometheus
- Splunk https://www.splunk.com/en_us/blog/devops/metrics-from-prometheus-exporters-are-now-available-with-the-sfx-smart-agent.html
